{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "#!pip install pyspark\n#!pip install --upgrade google-cloud-bigquery[pandas]\n#!pip install pyspark[sql] #PARSEO RAPIDO DE PANDAS A SPARK RDDSQL"}, {"cell_type": "markdown", "metadata": {}, "source": "### Importing the libraries and starting the Spark Session"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext \nfrom pyspark.sql import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport pandas as pd"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "#!export ARROW_PRE_0_15_IPC_FORMAT=1\n#!echo $ARROW_PRE_0_15_IPC_FORMAT"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "sc = SparkContext.getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "spark = SparkSession(sc)\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "#from pyspark.context import \nspark = SparkSession(sc)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Add asset from remote connection "}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "final_stat = None"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "SELECT USERID as ID_CTE, ID_FAM as ID_CLAS1, FREQUENCY as FREQUENCY\nFROM `rmf2gcp.RawData.Workflow_aggregado`\nWHERE id_table_dem <= 1708713\n"}], "source": "# Get data from GCP- BQ\nfrom google.cloud import bigquery\nimport time\nt0 = time.time()\n\nporcentaje = 9\n\nlimite = int(189857 * porcentaje)\n\ndef get_data_BQ(sql):\n    client = bigquery.Client()\n    df = client.query(sql).to_dataframe()\n    return(df)\nsql =  '''SELECT USERID as ID_CTE, ID_FAM as ID_CLAS1, FREQUENCY as FREQUENCY\nFROM `rmf2gcp.RawData.Workflow_aggregado`\nWHERE id_table_dem <= ''' + str(limite) #310 588 606 ''' # corre en mi local y pesa 56MB %1 del total de la muestra\nprint(sql)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "final_stat = get_data_BQ(sql)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ID_CTE       int64\nID_CLAS1     int64\nFREQUENCY    int64\ndtype: object\n(32639220, 3)\n"}], "source": "print(final_stat.dtypes)\nprint(final_stat.shape)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n: java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$readArrowStreamFromFile$2(ArrowConverters.scala:216)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n  warnings.warn(msg)\n"}, {"name": "stdout", "output_type": "stream", "text": "+-------+--------+---------+\n| ID_CTE|ID_CLAS1|FREQUENCY|\n+-------+--------+---------+\n|7502746|  103079|        8|\n|4393818|  313152|        6|\n|7958247|  380283|        7|\n|1574064|  380283|        7|\n|7913849|  106061|        6|\n+-------+--------+---------+\nonly showing top 5 rows\n\n"}], "source": "final_stat = spark.createDataFrame(final_stat)\nfinal_stat.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n"}], "source": "final_stat.count()\nprint(type(final_stat))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Preparing data for the model"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ratings = (final_stat\n    .select(\n        'ID_CTE',\n        'ID_CLAS1',\n        'FREQUENCY',\n    )\n).cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Spliting the data set to test and train for measuring the performance of the ALS Model"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "(training, test) = ratings.randomSplit([0.8, 0.2])"}, {"cell_type": "markdown", "metadata": {}, "source": "### Build the recommendation model using ALS on the training data\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Root-mean-square error = 1.5462754409847685\n"}], "source": "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=2, regParam=0.01, \n          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=True)\n\nmodel = als.fit(ratings)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n                                predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+--------+---------+------------+\n| ID_CTE|ID_CLAS1|FREQUENCY|  prediction|\n+-------+--------+---------+------------+\n|4056813|  212010|        1|4.5328977E-4|\n|8226344|  212010|        1|6.6993537E-4|\n|6301469|  212010|        1|1.8051936E-4|\n|8203832|  212010|        1|  5.51962E-4|\n|5656025|  212010|        1| 3.318036E-4|\n+-------+--------+---------+------------+\nonly showing top 5 rows\n\n"}], "source": "predictions.show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Parameters of ALS Model in PySpark realization are following:\n\n##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n##### rank is the number of latent factors in the model.\n##### maxIter is the maximum number of iterations to run.\n##### regParam specifies the regularization parameter in ALS.\n##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"}, {"cell_type": "markdown", "metadata": {}, "source": "###  Generate top 10 Item recommendations for each user\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1708713\n+------+--------------------+\n|ID_CTE|     recommendations|\n+------+--------------------+\n| 10362|[[224025, 0.05139...|\n| 11033|[[701305, 0.53538...|\n| 11141|[[105068, 0.37909...|\n| 12940|[[380283, 0.77650...|\n| 13832|[[318009, 0.55853...|\n+------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "userRecs = model.recommendForAllUsers(10)\nprint(userRecs.count())\nuserRecs.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(ID_CTE=10362, recommendations=[Row(ID_CLAS1=224025, rating=0.05139085650444031), Row(ID_CLAS1=229022, rating=0.04640220105648041), Row(ID_CLAS1=224041, rating=0.04161088541150093), Row(ID_CLAS1=102089, rating=0.03968541696667671), Row(ID_CLAS1=102016, rating=0.03477980196475983), Row(ID_CLAS1=290059, rating=0.03472079336643219), Row(ID_CLAS1=102164, rating=0.031431473791599274), Row(ID_CLAS1=701305, rating=0.030639003962278366), Row(ID_CLAS1=229035, rating=0.030155781656503677), Row(ID_CLAS1=102010, rating=0.02981329895555973)]),\n Row(ID_CTE=11033, recommendations=[Row(ID_CLAS1=701305, rating=0.5353883504867554), Row(ID_CLAS1=313064, rating=0.38923436403274536), Row(ID_CLAS1=314064, rating=0.35390523076057434), Row(ID_CLAS1=862009, rating=0.34793874621391296), Row(ID_CLAS1=381009, rating=0.3431849181652069), Row(ID_CLAS1=867047, rating=0.3318749964237213), Row(ID_CLAS1=314129, rating=0.2766352593898773), Row(ID_CLAS1=102089, rating=0.25155359506607056), Row(ID_CLAS1=423132, rating=0.2473699450492859), Row(ID_CLAS1=774253, rating=0.23692163825035095)])]"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "userRecs.take(2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|     recommendations|\n+--------------------+\n|[[224025, 0.05139...|\n|[[701305, 0.53538...|\n|[[105068, 0.37909...|\n|[[380283, 0.77650...|\n|[[318009, 0.55853...|\n|[[862009, 0.38653...|\n|[[381009, 0.55220...|\n|[[867048, 0.28203...|\n|[[224073, 0.67350...|\n|[[701305, 1.40924...|\n|[[314156, 0.65680...|\n|[[313152, 0.64131...|\n|[[295019, 0.15151...|\n|[[102089, 0.12812...|\n|[[701305, 0.37526...|\n|[[229032, 0.61908...|\n|[[314156, 0.63714...|\n|[[106055, 0.33072...|\n|[[701305, 1.11097...|\n|[[380073, 0.59425...|\n+--------------------+\nonly showing top 20 rows\n\n"}], "source": "userRecs[['recommendations']].show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "1"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "1"}, {"cell_type": "markdown", "metadata": {}, "source": "### Display the recommendations and get them in the correct format"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+--------------------+\n|ID_CTE|     recommendations|\n+------+--------------------+\n| 10362|[224025, 0.051390...|\n| 10362| [229022, 0.0464022]|\n| 10362|[224041, 0.041610...|\n| 10362|[102089, 0.039685...|\n+------+--------------------+\nonly showing top 4 rows\n\n"}], "source": "from pyspark.sql.functions import explode\nuserRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\nuserRecs1.show(4)"}, {"cell_type": "markdown", "metadata": {}, "source": "####  Breaking down reach recommendation to separate columns"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "userRecs1= userRecs1.select('ID_CTE', 'recommendations.*')       "}, {"cell_type": "markdown", "metadata": {}, "source": "### Display the results"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+--------+-----------+\n|ID_CTE|ID_CLAS1|     rating|\n+------+--------+-----------+\n| 10362|  224025|0.051390857|\n| 10362|  229022|  0.0464022|\n+------+--------+-----------+\nonly showing top 2 rows\n\n"}], "source": "userRecs1.show(2) "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "17087130"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "userRecs1.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Writing the Output back to the Remote Datasource"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[ID_CTE: int, ID_CLAS1: int, rating: float]"}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": "final_stat = userRecs1.toPandas()\nuserRecs1.unpersist(True)"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Collecting pandas_gbq\n  Downloading https://files.pythonhosted.org/packages/53/f3/3100eb9332c62c5e5ac486d5421965da23a0b92012825bfbb372b7f8d508/pandas_gbq-0.13.2-py3-none-any.whl\nRequirement already satisfied: google-cloud-bigquery>=1.11.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.25.0)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (41.4.0)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (0.4.1)\nCollecting pydata-google-auth (from pandas_gbq)\n  Downloading https://files.pythonhosted.org/packages/0b/dc/be321b769b761ec2640f1e4561c2953dd6a4a3efe6b10b5781774c71177a/pydata_google_auth-1.1.0-py2.py3-none-any.whl\nRequirement already satisfied: pandas>=0.19.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.0.4)\nRequirement already satisfied: google-auth in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.17.2)\nRequirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.20.0)\nRequirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (0.5.1)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (3.12.2)\nRequirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.15.0)\nRequirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.3.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth-oauthlib->pandas_gbq) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (2.8.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (1.17.2)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (2019.3)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (4.6)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (4.1.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (0.2.8)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (1.52.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (2.23.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (3.1.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/anaconda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth->pandas_gbq) (0.4.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (1.24.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (2019.9.11)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (2.8)\nInstalling collected packages: pydata-google-auth, pandas-gbq\nSuccessfully installed pandas-gbq-0.13.2 pydata-google-auth-1.1.0\n"}], "source": "!pip install pandas_gbq"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "'Resultados.test_spark_09porciento_17_junio_2020'"}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "table_id = 'Resultados.test_spark_0'+str(porcentaje)+'porciento_17_junio_2020'\ntable_id"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "1it [03:39, 219.13s/it]"}, {"name": "stdout", "output_type": "stream", "text": "3510.216548681259\n"}, {"name": "stderr", "output_type": "stream", "text": "\n"}], "source": "final_stat.to_gbq(table_id, project_id='rmf2gcp')\nt3 = time.time()\ntotal = t3-t0\nprint(total)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!mkdir test/"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#final_stat.to_csv('test_spark_0'+str(porcentaje)+'porciento_17_junio_2020')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!gsutil cp test_gcp_cluster_10_junio_2020.csv gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!zip test_gcp_cluster_10_junio_2020.csv.zip test_gcp_cluster_10_junio_2020.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!ls"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!gsutil cp test_gcp_cluster_10_junio_2020.csv.zip gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv.zip"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "##!rm -r test_modelos\n#!mkdir test_modelos_gcp\n#!chmod 777 test_modelos_gcp"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#from pyspark.ml import Pipeline"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#pipeline = Pipeline(stages=[model])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#model_alsWML = pipeline.fit(ratings)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#model_alsWML.save('/test_modelos_gcp/')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!ls -la"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!gsutil cp test_gcp_cluster_10_junio_2020.csv.zip gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv.zip"}], "metadata": {"environment": {"name": "tf-gpu.1-15.m48", "type": "gcloud", "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m48"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}