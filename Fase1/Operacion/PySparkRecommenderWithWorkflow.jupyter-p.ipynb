{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Workflow automation for a recommender engine using ALS Model in PySpark\n### This notebook demonstrates the automation of a product recommendation engine given we have user-item interaction data in terms of the frequency of purchase for each unique user-item pair\n### The automation is implemented by deploying the notebook as a job which reads the input data from a remote database, trains and runs the model and writes the output back to the designated database\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Importing the libraries and starting the Spark Session"}, {"metadata": {}, "cell_type": "code", "source": "import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext \nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc =SparkContext.getOrCreate()", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark = SparkSession(sc)", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Remote connection to the database ( Any supported database like DB2, Netezza, PDA )\n### To enable the connection, add your database under the Data Sources tab in your project. You would need information about your database like JDBC URL, type, username/password\n\n### Adding the asset (for example- the dataset which has the user-item interaction information) from the remote database after setting up the connection"}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# This connection object is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\nfrom project_lib import Project\nproject = Project.access()\nPDA_COPPEL_2020_credentials = project.get_connection(name=\"PDA_DIRECCIONRIESGOS\")\n\nimport jaydebeapi, pandas as pd\nPDA_COPPEL_2020_connection = jaydebeapi.connect('org.netezza.Driver',\n    '{}://{}:{}/{}'.format('jdbc:netezza',\n    PDA_COPPEL_2020_credentials['host'],\n    PDA_COPPEL_2020_credentials['port'],\n    PDA_COPPEL_2020_credentials['database']),\n    [PDA_COPPEL_2020_credentials['username'],\n    PDA_COPPEL_2020_credentials['password']])\n\n#query = 'SELECT CLIENTECODIGO as ID_CTE, ADCLAFAM as ID_CLAS1, FRECUENCIA as FREQUENCY  FROM(SELECT *,TRIM(TO_CHAR(FAMILIA,\"000\")) AS FAM,TRIM(TO_CHAR(CLASE,\"00\")) AS CLAS,TRIM(TO_CHAR(AREA,\"0\")) AS AREA,(AREA||DEPARTAMENTO||CLAS||FAM) AS ADCLAFAM FROM(SELECT  CLIENTECODIGO,CLASE, FAMILIA, DEPARTAMENTO,SUM(CASE WHEN CLASE>\"0\" THEN 1 ELSE 0 END) AS FRECUENCIA,MAX(CASE WHEN CARTERA=\"Ropa\" THEN 1  WHEN CARTERA=\"Muebles\" THEN 2 WHEN CARTERA=\"Prestamos\" THEN 3 ELSE 0 END) AS AREA FROM(SELECT *,CASE WHEN CLASE>\"0\" THEN 1 ELSE 0 END AS T_CLASE,CASE WHEN FAMILIA>\"0\" THEN 1 ELSE 0 END AS T_FAMILIA FROM DIRECCIONRIESGOS.ADMIN.TRANSACCIONESCARTERAS where FECHACORTE between \"2017-01-31\" and \"2019-12-31\" and CLIENTECODIGO not in (9001,9000) AND CLIENTECODIGO >10000) E GROUP BY CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO ORDER BY CLIENTECODIGO) T WHERE CLASE NOT IN (-99)) P ORDER BY CLIENTECODIGO,ADCLAFAM'\nquery = 'SELECT * FROM DIRECCIONRIESGOS.ADMIN.TRANSACCIONESCARTERAS limit 10'\ndata_df_1 = pd.read_sql(query, con=PDA_COPPEL_2020_connection)\ndata_df_1.head(10)", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CLIENTECODIGO</th>\n      <th>FECHACORTE</th>\n      <th>FECHACOMPRA</th>\n      <th>FACTURA</th>\n      <th>CODIGO</th>\n      <th>TIPOCOMPRA</th>\n      <th>CARTERA</th>\n      <th>CATEGORIA</th>\n      <th>DESCATEGORIA</th>\n      <th>DEPARTAMENTO</th>\n      <th>...</th>\n      <th>DESFAMILIA</th>\n      <th>CANTIDAD</th>\n      <th>PRECIO_VTA</th>\n      <th>DESCUENTO</th>\n      <th>ENGANCHE</th>\n      <th>INTERES</th>\n      <th>IMPORTE_IVA</th>\n      <th>PLAZOVENTA</th>\n      <th>NUM_TIE</th>\n      <th>NUM_CIUDAD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40655876</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-24 00:00:00</td>\n      <td>52377</td>\n      <td>317774</td>\n      <td>Credito</td>\n      <td>Ropa</td>\n      <td>11</td>\n      <td>Lencer\u00eda</td>\n      <td>3</td>\n      <td>...</td>\n      <td>BABY DOLL</td>\n      <td>1</td>\n      <td>137.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>137.0</td>\n      <td>8</td>\n      <td>6537</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-03 00:00:00</td>\n      <td>47196</td>\n      <td>180997</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>3</td>\n      <td>Caballero Interior</td>\n      <td>1</td>\n      <td>...</td>\n      <td>BILLETERA</td>\n      <td>1</td>\n      <td>154.0</td>\n      <td>0.0</td>\n      <td>179.0</td>\n      <td>0.0</td>\n      <td>-25.0</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>158</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-02 00:00:00</td>\n      <td>262726</td>\n      <td>318894</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>11</td>\n      <td>Lencer\u00eda</td>\n      <td>3</td>\n      <td>...</td>\n      <td>CONJ BRASSIERE SEXY</td>\n      <td>1</td>\n      <td>137.0</td>\n      <td>0.0</td>\n      <td>198.0</td>\n      <td>0.0</td>\n      <td>-61.0</td>\n      <td>0</td>\n      <td>941</td>\n      <td>404</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-30 00:00:00</td>\n      <td>668577</td>\n      <td>704056</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>5</td>\n      <td>Cuidado Personal</td>\n      <td>7</td>\n      <td>...</td>\n      <td>BUCAL INFANTIL</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>373</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-17 00:00:00</td>\n      <td>66304</td>\n      <td>145477</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>4</td>\n      <td>Caballero Exterior</td>\n      <td>1</td>\n      <td>...</td>\n      <td>ML FSIA MODA</td>\n      <td>1</td>\n      <td>258.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>258.0</td>\n      <td>0</td>\n      <td>6599</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>45457860</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-02 00:00:00</td>\n      <td>272141</td>\n      <td>306364</td>\n      <td>Credito</td>\n      <td>Ropa</td>\n      <td>11</td>\n      <td>Lencer\u00eda</td>\n      <td>3</td>\n      <td>...</td>\n      <td>CONJ BRASSIERE SEXY</td>\n      <td>1</td>\n      <td>146.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>146.0</td>\n      <td>8</td>\n      <td>393</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8374783</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-21 00:00:00</td>\n      <td>497740</td>\n      <td>314297</td>\n      <td>Credito</td>\n      <td>Ropa</td>\n      <td>11</td>\n      <td>Lencer\u00eda</td>\n      <td>3</td>\n      <td>...</td>\n      <td>PANTALETA</td>\n      <td>1</td>\n      <td>146.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>36.0</td>\n      <td>182.0</td>\n      <td>8</td>\n      <td>37</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-13 00:00:00</td>\n      <td>104762</td>\n      <td>555366</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>1</td>\n      <td>BEBES</td>\n      <td>5</td>\n      <td>...</td>\n      <td>CALCETA VESTIR NI\u00d1A</td>\n      <td>1</td>\n      <td>77.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>77.0</td>\n      <td>0</td>\n      <td>1364</td>\n      <td>588</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-25 00:00:00</td>\n      <td>167530</td>\n      <td>728005</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>21</td>\n      <td>Impulso</td>\n      <td>7</td>\n      <td>...</td>\n      <td>BOLSA REGALO</td>\n      <td>1</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>128.0</td>\n      <td>0.0</td>\n      <td>-112.0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9000</td>\n      <td>2020-01-31 00:00:00</td>\n      <td>2020-01-10 00:00:00</td>\n      <td>522577</td>\n      <td>807564</td>\n      <td>Contado</td>\n      <td>Ropa</td>\n      <td>18</td>\n      <td>Deportivo</td>\n      <td>8</td>\n      <td>...</td>\n      <td>CORRER</td>\n      <td>1</td>\n      <td>344.0</td>\n      <td>0.0</td>\n      <td>488.0</td>\n      <td>0.0</td>\n      <td>-144.0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>17</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows \u00d7 24 columns</p>\n</div>", "text/plain": "   CLIENTECODIGO           FECHACORTE          FECHACOMPRA  FACTURA  CODIGO  \\\n0       40655876  2020-01-31 00:00:00  2020-01-24 00:00:00    52377  317774   \n1           9000  2020-01-31 00:00:00  2020-01-03 00:00:00    47196  180997   \n2           9000  2020-01-31 00:00:00  2020-01-02 00:00:00   262726  318894   \n3           9000  2020-01-31 00:00:00  2020-01-30 00:00:00   668577  704056   \n4           9000  2020-01-31 00:00:00  2020-01-17 00:00:00    66304  145477   \n5       45457860  2020-01-31 00:00:00  2020-01-02 00:00:00   272141  306364   \n6        8374783  2020-01-31 00:00:00  2020-01-21 00:00:00   497740  314297   \n7           9000  2020-01-31 00:00:00  2020-01-13 00:00:00   104762  555366   \n8           9000  2020-01-31 00:00:00  2020-01-25 00:00:00   167530  728005   \n9           9000  2020-01-31 00:00:00  2020-01-10 00:00:00   522577  807564   \n\n  TIPOCOMPRA CARTERA CATEGORIA        DESCATEGORIA  DEPARTAMENTO  ...  \\\n0    Credito    Ropa        11            Lencer\u00eda             3  ...   \n1    Contado    Ropa         3  Caballero Interior             1  ...   \n2    Contado    Ropa        11            Lencer\u00eda             3  ...   \n3    Contado    Ropa         5    Cuidado Personal             7  ...   \n4    Contado    Ropa         4  Caballero Exterior             1  ...   \n5    Credito    Ropa        11            Lencer\u00eda             3  ...   \n6    Credito    Ropa        11            Lencer\u00eda             3  ...   \n7    Contado    Ropa         1               BEBES             5  ...   \n8    Contado    Ropa        21             Impulso             7  ...   \n9    Contado    Ropa        18           Deportivo             8  ...   \n\n              DESFAMILIA  CANTIDAD PRECIO_VTA  DESCUENTO ENGANCHE  INTERES  \\\n0  BABY DOLL                     1      137.0        0.0      0.0      0.0   \n1  BILLETERA                     1      154.0        0.0    179.0      0.0   \n2  CONJ BRASSIERE SEXY           1      137.0        0.0    198.0      0.0   \n3  BUCAL INFANTIL                1       22.0        0.0      0.0      0.0   \n4  ML FSIA MODA                  1      258.0        0.0      0.0      0.0   \n5  CONJ BRASSIERE SEXY           1      146.0        0.0      0.0      0.0   \n6  PANTALETA                     1      146.0        0.0      0.0     36.0   \n7  CALCETA VESTIR NI\u00d1A           1       77.0        0.0      0.0      0.0   \n8  BOLSA REGALO                  1       16.0        0.0    128.0      0.0   \n9  CORRER                        1      344.0        0.0    488.0      0.0   \n\n   IMPORTE_IVA  PLAZOVENTA  NUM_TIE  NUM_CIUDAD  \n0        137.0           8     6537          38  \n1        -25.0           0     1000         158  \n2        -61.0           0      941         404  \n3         22.0           0      373          46  \n4        258.0           0     6599          71  \n5        146.0           8      393           1  \n6        182.0           8       37          42  \n7         77.0           0     1364         588  \n8       -112.0           0       21           8  \n9       -144.0           0       10          17  \n\n[10 rows x 24 columns]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "\n# @hidden_cell\n# This connection object is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\nfrom project_lib import Project\nproject = Project.access()\nPDA_COPPEL_2020_credentials = project.get_connection(name=\"PDA_COPPEL_2020\")\n\nfrom pyspark.sql import SparkSession\nsparkSession = SparkSession(spark).builder.getOrCreate()\n\ndf = sparkSession.read.format('jdbc') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'],PDA_COPPEL_2020_credentials['database'])) \\\n    .option('dbtable', 'USER_COPPEL_2020.MODELO_RECOMENDACION') \\\n    .option('user', PDA_COPPEL_2020_credentials['username']) \\\n    .option('password', PDA_COPPEL_2020_credentials['password']).load\ndf.show(5)\n\n", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Missing personal credentials for \"PDA_COPPEL_2020\" (c28e7213-e2dc-42cc-aeb7-90cb4a526e0c).\nPlease configure them in the Watson Studio project.\n", "name": "stderr"}, {"output_type": "error", "ename": "KeyError", "evalue": "'username'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", "\u001b[0;32m<ipython-input-5-50935dfa8f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msparkSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jdbc'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jdbc:netezza://{}:{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDA_COPPEL_2020_credentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPDA_COPPEL_2020_credentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'port'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPDA_COPPEL_2020_credentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'database'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dbtable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USER_COPPEL_2020.MODELO_RECOMENDACION'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPDA_COPPEL_2020_credentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPDA_COPPEL_2020_credentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyError\u001b[0m: 'username'"]}]}, {"metadata": {}, "cell_type": "code", "source": "import project_lib\nproject_lib.Project.get_file(\"TRANSACTIONS\")", "execution_count": 8, "outputs": [{"output_type": "error", "ename": "TypeError", "evalue": "get_file() missing 1 required positional argument: 'file_name'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-8-f3b72df7a8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproject_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproject_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRANSACTIONS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mTypeError\u001b[0m: get_file() missing 1 required positional argument: 'file_name'"]}]}, {"metadata": {}, "cell_type": "code", "source": "#import dsx_core_utils, requests, os, io\n#!pip install --proxy=https://10.33.128.80:8080 dsx\n#!pip install --proxy=https://10.33.128.80:8080 dsx\n\n#!pip install --proxy=https://10.33.128.80:8080 dsx\nimport dsx_core_utils\ndataSet = dsx_core_utils.get_remote_data_set_info('TRANSACTIONS')\n#final_stat = None\n#dataSet = dsx_core_utils.get_remote_data_set_info('TRANSACTIONS')\n#dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])3\n#sparkSession = SparkSession(sc).builder.getOrCreate()\n# Load JDBC data to Spark dataframe\n#dbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\n#if (dataSet['query']):\n#    dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\n#final_stat = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\", dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\n#final_stat.show(5)", "execution_count": 12, "outputs": [{"output_type": "error", "ename": "AttributeError", "evalue": "module 'dsx' has no attribute 'get_remote_data_set_info'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m<ipython-input-12-772fd5298921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#!pip install --proxy=https://10.33.128.80:8080 dsx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_remote_data_set_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TRANSACTIONS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#final_stat = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#dataSet = dsx_core_utils.get_remote_data_set_info('TRANSACTIONS')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: module 'dsx' has no attribute 'get_remote_data_set_info'"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Preparing data for the model"}, {"metadata": {}, "cell_type": "code", "source": "ratings = (final_stat\n    .select(\n        'ID_CTE',\n        'ID_CLAS1',\n        'FREQUENCY',\n    )\n).cache()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Make sure your data is 'integer' type "}, {"metadata": {}, "cell_type": "code", "source": "ratings.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ratings.limit(10000)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Spliting the data set to test and train for measuring the performance of the ALS Model"}, {"metadata": {}, "cell_type": "code", "source": "(training, test) = ratings.randomSplit([0.8, 0.2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Build the recommendation model using ALS on the training data\n### Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Parameters of ALS Model in PySpark realization are following:\n\n##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n##### rank is the number of latent factors in the model.\n##### maxIter is the maximum number of iterations to run.\n##### regParam specifies the regularization parameter in ALS.\n##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Explicit vs. implicit feedback\n#### The standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to products.\n#### It is common in many real-world use cases to only have access to implicit feedback (e.g. views, clicks, purchases, likes, shares etc.). The approach used in spark.ml to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets. Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item."}, {"metadata": {}, "cell_type": "code", "source": "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=2, regParam=0.01, \n          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=True)\n\nmodel = als.fit(ratings)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n                                predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "###  Generate top k Item recommendations for each user\n#### The value of 'k' here is 10 and can be changed by passing the desired value to the function\n\n"}, {"metadata": {}, "cell_type": "code", "source": "userRecs = model.recommendForAllUsers(10)\nuserRecs.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Display the results : Each row represents the 'k' recommendations for each User"}, {"metadata": {}, "cell_type": "code", "source": "userRecs.take(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### For getting each recommendation as a row in the final csv, we break down the result generated above using the explode function"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import explode\n#userRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\n#userRecs1.show()", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "####  Breaking down reach recommendation to separate columns"}, {"metadata": {}, "cell_type": "code", "source": "import select as s\n", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nuserRecs1= userRecs1 \\\n  .select('ID_CTE', 'recommendations.*')    \n   ", "execution_count": 10, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'userRecs1' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-eac2f56c689a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muserRecs1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0muserRecs1\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ID_CTE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recommendations.*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'userRecs1' is not defined"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Display the results"}, {"metadata": {}, "cell_type": "code", "source": "userRecs1.show() ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Writing the Output back to the Remote Datasource\n#### Thereby the output or resulting csv can be consumed directly by anyone who has the access to the remote database "}, {"metadata": {}, "cell_type": "code", "source": "new_table_name = 'RecommendationsResult'\nuserRecs1.coalesce(1).write \\\n   .format(\"jdbc\") \\\n    .mode('overwrite') \\\n    .option(\"url\", dataSource['URL']) \\\n    .option(\"dbtable\", dataSet['schema']+\".\"+new_table_name) \\\n    .option(\"user\", dataSource['user']) \\\n    .option(\"password\", dataSource['password']) \\\n    .save()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Saving the model for deployment in WML"}, {"metadata": {}, "cell_type": "code", "source": "from dsx_ml.ml import save\n", "execution_count": 11, "outputs": [{"output_type": "error", "ename": "ModuleNotFoundError", "evalue": "No module named 'dsx_ml'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-11-3392444c28ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdsx_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dsx_ml'"]}]}, {"metadata": {}, "cell_type": "code", "source": "type(model)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Pack the model inside a pipeline \n#### Since the WML deployments allow saving Spark Pipelines directly, put the ALS model inside a Pipeline for direct deployment stage\n#### Typically, A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For this case, since the pipeline is bought in to action just for the sole cause of deployment, we do not use any transformers and such"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml import Pipeline", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(stages=[model])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_alsWML = pipeline.fit(ratings)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "save(name = 'PySparkRecommenderPipeline',\n     model = model_alsWML,\n     test_data = ratings,\n     algorithm_type = 'Classification',\n     source='PySparkRecommenderWithWorkflow.ipynb',\n     description='Recommender using PySpark')", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 2}