{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Workflow automation for a recommender engine using ALS Model in PySpark\n### This notebook demonstrates the automation of a product recommendation engine given we have user-item interaction data in terms of the frequency of purchase for each unique user-item pair\n### The automation is implemented by deploying the notebook as a job which reads the input data from a remote database, trains and runs the model and writes the output back to the designated database\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Importing the libraries and starting the Spark Session"}, {"metadata": {}, "cell_type": "code", "source": "### Load packages \nimport time\nt0 = time.time()\nimport pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel # factorizacion de matrices\nfrom pyspark.context import SparkContext \nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport jaydebeapi, pandas as pd", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200918213828-0004\nKERNEL_ID = 56083c30-7ef9-48aa-a201-9238e70c0f1e\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "### set up spark session and context \nsc = SparkContext.getOrCreate()\nspark = SparkSession(sc)", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Remote connection to the database ( Any supported database like DB2, Netezza, PDA )\n### To enable the connection, add your database under the Data Sources tab in your project. You would need information about your database like JDBC URL, type, username/password\n\n### Adding the asset (for example- the dataset which has the user-item interaction information) from the remote database after setting up the connection"}, {"metadata": {}, "cell_type": "code", "source": "# Access to Porject's conexion\n# and get the data \nfrom project_lib import Project\nproject = Project.access()\nPDA_COPPEL_2020_credentials = project.get_connection(name=\"PDA_Analytics\")\n\nPDA_COPPEL_2020_connection = jaydebeapi.connect('org.netezza.Driver',\n    '{}://{}:{}/{}'.format('jdbc:netezza',\n    PDA_COPPEL_2020_credentials['host'],\n    PDA_COPPEL_2020_credentials['port'],\n    PDA_COPPEL_2020_credentials['database']),\n    [PDA_COPPEL_2020_credentials['username'],\n    PDA_COPPEL_2020_credentials['password']])\n\nquery = '''\nSELECT CLIENTECODIGO as ID_CTE, CAST( ADCLAFAM as integer) as ID_CLAS1, \n\t\tFRECUENCIA as FREQUENCY  \n\tFROM(\tSELECT *, TRIM(TO_CHAR(FAMILIA,'000')) AS FAM, TRIM(TO_CHAR(CLASE,'00')) AS CLAS, \n\t                  TRIM(TO_CHAR(AREA,'0')) AS AREA, (AREA||DEPARTAMENTO||CLAS||FAM) AS ADCLAFAM \n\t\t\t\tFROM(    SELECT  CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO, \n\t\t\t\t                 SUM(CASE WHEN CLASE>'0' THEN 1 ELSE 0 END) AS FRECUENCIA,\n\t\t\t\t                 MAX(CASE WHEN CARTERA='Ropa' THEN 1  \n\t\t\t\t\t\t\t\t          WHEN CARTERA='Muebles' THEN 2\n\t\t\t\t\t\t\t\t\t\t  WHEN CARTERA='Prestamos' THEN 3 ELSE 0 END) AS AREA\n\t\t\t\t\t\t\tFROM( SELECT *, CASE WHEN CLASE>'0' THEN 1 ELSE 0 END AS T_CLASE, \n\t\t\t\t\t\t\t               CASE WHEN FAMILIA>'0' THEN 1 ELSE 0 END AS T_FAMILIA\n\t\t\t\t\t\t\t\t\tFROM DIRECCIONRIESGOS.ADMIN.TRANSACCIONESCARTERAS \n\t\t\t\t\t\t\t\t\twhere FECHACORTE between '2017-01-31' and '2019-12-31' \n\t\t\t\t\t\t\t\t\tand CLIENTECODIGO not in (9001,9000) AND CLIENTECODIGO >10000) E \n\t\t\t\tGROUP BY CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO \n\t\t\t\tORDER BY CLIENTECODIGO) T \n\tWHERE CLASE NOT IN (-99)) P \n\tORDER BY CLIENTECODIGO,ADCLAFAM limit 318828\n'''\n# 1% ~ 40 minutos \n# 0.1% ~ 6 minutos \n# medimos el tiempo en transferir los datos \nt1 = time.time()\ndata_df_1 = pd.read_sql(query, con=PDA_COPPEL_2020_connection)\nt2 = time.time()\nseconds_get_data = t2 - t1 \nprint(str(seconds_get_data / 60 ) + ' minutos en transferir tabla')\ndata_df_1.head(10)", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "2.9779995759328206 minutos en transferir tabla\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 3, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_CTE</th>\n      <th>ID_CLAS1</th>\n      <th>FREQUENCY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10002</td>\n      <td>1313064</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10002</td>\n      <td>1314064</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10002</td>\n      <td>1862008</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10002</td>\n      <td>1867048</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10002</td>\n      <td>2210070</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10002</td>\n      <td>2222011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10002</td>\n      <td>2229008</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10002</td>\n      <td>2244001</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10002</td>\n      <td>2248001</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10002</td>\n      <td>2665020</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   ID_CTE  ID_CLAS1  FREQUENCY\n0   10002   1313064        1.0\n1   10002   1314064        3.0\n2   10002   1862008        4.0\n3   10002   1867048        2.0\n4   10002   2210070        1.0\n5   10002   2222011        1.0\n6   10002   2229008        1.0\n7   10002   2244001        1.0\n8   10002   2248001        1.0\n9   10002   2665020        1.0"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "print(data_df_1.dtypes) # #  validamos el tipo de dato y numero de registros\nprint(data_df_1.shape)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "ID_CTE         int64\nID_CLAS1       int64\nFREQUENCY    float64\ndtype: object\n(318828, 3)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark_df = spark.createDataFrame(data_df_1) # distribuimos el pandas dataframe a spark \n# hacer el get desde spark ", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# spark_df.show() #  no es necesario verlo ", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Preparing data for the model"}, {"metadata": {}, "cell_type": "code", "source": "ratings = (spark_df\n    .select(\n        'ID_CTE',\n        'ID_CLAS1',\n        'FREQUENCY',\n    )\n).cache() #  lo cargamos a memoria ram ", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Make sure your data is 'integer' type "}, {"metadata": {}, "cell_type": "markdown", "source": "### Spliting the data set to test and train for measuring the performance of the ALS Model"}, {"metadata": {}, "cell_type": "code", "source": "(training, test) = ratings.randomSplit([0.8, 0.2]) #  debemos validar esto ", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Build the recommendation model using ALS on the training data\n### Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Parameters of ALS Model in PySpark realization are following:\n\n##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n##### rank is the number of latent factors in the model.\n##### maxIter is the maximum number of iterations to run.\n##### regParam specifies the regularization parameter in ALS.\n##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Explicit vs. implicit feedback\n#### The standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to products.\n#### It is common in many real-world use cases to only have access to implicit feedback (e.g. views, clicks, purchases, likes, shares etc.). The approach used in spark.ml to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets. Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item."}, {"metadata": {}, "cell_type": "code", "source": "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nt2 = time.time()\nals = ALS(maxIter=10, regParam=0.01, rank = 100, \n          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=True)\n\nmodel = als.fit(ratings)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n                                predictionCol=\"prediction\")\nt3 = time.time()\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\nprint('Tiempo de entranar y predecir    ' + str( (t3- t2)/60) + '    minutos') ", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Root-mean-square error = 3.910787279063556\nTiempo de entranar y predecir    0.7275614261627197    minutos\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "###  Generate top k Item recommendations for each user\n#### The value of 'k' here is 10 and can be changed by passing the desired value to the function\n\n"}, {"metadata": {}, "cell_type": "code", "source": "userRecs = model.recommendForAllUsers(10)\nuserRecs.count()", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "17407"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Display the results : Each row represents the 'k' recommendations for each User"}, {"metadata": {}, "cell_type": "code", "source": "#userRecs.take(10)", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### For getting each recommendation as a row in the final csv, we break down the result generated above using the explode function"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import explode\nuserRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\nuserRecs1.show()", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "+------+--------------------+\n|ID_CTE|     recommendations|\n+------+--------------------+\n| 18911|[2968001, 0.9675391]|\n| 18911|[2920001, 0.7479556]|\n| 18911|[2910001, 0.5792166]|\n| 18911|[2990003, 0.5360558]|\n| 18911|[2990001, 0.52307...|\n| 18911|[2975001, 0.47655...|\n| 18911|[2950003, 0.22225...|\n| 18911|[2901001, 0.20224...|\n| 18911|[2920004, 0.18126...|\n| 18911|[2901004, 0.15629...|\n| 37111|[1318203, 1.0439721]|\n| 37111|[1314063, 1.0095295]|\n| 37111|[1860048, 0.97131...|\n| 37111|[1102016, 0.96551...|\n| 37111|[1318003, 0.9465939]|\n| 37111|[1862009, 0.9254286]|\n| 37111|[1102012, 0.9153261]|\n| 37111|[1318073, 0.8597628]|\n| 37111|[2121001, 0.8550992]|\n| 37111|[1646081, 0.8025979]|\n+------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "####  Breaking down reach recommendation to separate columns"}, {"metadata": {}, "cell_type": "code", "source": "#import select as s   PDA_Analytics\n", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "userRecs1= userRecs1 \\\n  .select('ID_CTE', 'recommendations.*')       ", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Writing the Output back to the Remote Datasource\n#### Thereby the output or resulting csv can be consumed directly by anyone who has the access to the remote database "}, {"metadata": {}, "cell_type": "code", "source": "'''new_table_name = 'ANALITICAAFORE.ADMIN.RECOMMENDATIONSRESULT_TEST_CENIC'\n\nuserRecs1.coalesce(2).write.format('jdbc') \\\n    .mode('overwrite') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'], PDA_COPPEL_2020_credentials['database'])) \\\n    .option('dbtable', new_table_name) \\\n    .option('user', PDA_COPPEL_2020_credentials['username']).option('driver','org.netezza.Driver').option('password', PDA_COPPEL_2020_credentials['password']).save()\n'''    ", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "\"new_table_name = 'ANALITICAAFORE.ADMIN.RECOMMENDATIONSRESULT_TEST_CENIC'\\n\\nuserRecs1.coalesce(2).write.format('jdbc')     .mode('overwrite')     .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'], PDA_COPPEL_2020_credentials['database']))     .option('dbtable', new_table_name)     .option('user', PDA_COPPEL_2020_credentials['username']).option('driver','org.netezza.Driver').option('password', PDA_COPPEL_2020_credentials['password']).save()\\n\""}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "userRecs1.show(5)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "+------+--------+----------+\n|ID_CTE|ID_CLAS1|    rating|\n+------+--------+----------+\n| 18911| 2968001| 0.9675391|\n| 18911| 2920001| 0.7479556|\n| 18911| 2910001| 0.5792166|\n| 18911| 2990003| 0.5360558|\n| 18911| 2990001|0.52307117|\n+------+--------+----------+\nonly showing top 5 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "new_table_name = 'RecommendationsResult_test_cenic'\n\nuserRecs1.coalesce(1).write.format('jdbc') \\\n    .mode('overwrite') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'],'CENIC')) \\\n    .option('dbtable', new_table_name) \\\n    .option('user', PDA_COPPEL_2020_credentials['username']).option('driver','org.netezza.Driver').option('password', PDA_COPPEL_2020_credentials['password']).save()", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Saving the model for deployment in WML"}, {"metadata": {}, "cell_type": "code", "source": "#!pip install  --proxy=https://10.33.128.80:8080 dsx proxy para usar pip ", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Pack the model inside a pipeline \n#### Since the WML deployments allow saving Spark Pipelines directly, put the ALS model inside a Pipeline for direct deployment stage\n#### Typically, A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For this case, since the pipeline is bought in to action just for the sole cause of deployment, we do not use any transformers and such"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml import Pipeline", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(stages=[model])", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_alsWML = pipeline.fit(ratings)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# model_alsWML.save('/temp/')", "execution_count": 22, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "t_final = time.time()", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(t_final- t0)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "369.10957765579224\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "print((t_final- t0) / 60 )", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "6.151826294263204\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 2}