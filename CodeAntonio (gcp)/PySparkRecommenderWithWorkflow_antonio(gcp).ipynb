{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow demonstration with a recommender engine on a sampled dataset from Transactions.csv using ALS Model\n",
    "### This is the notebook for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries and starting the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!export ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "!echo $ARROW_PRE_0_15_IPC_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.context import \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add asset from remote connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from GCP- BQ\n",
    "from google.cloud import bigquery\n",
    "def get_data_BQ(sql):\n",
    "    client = bigquery.Client()\n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return(df)\n",
    "sql =  '''SELECT ID_CTE as ID_CTE, ID_FAM as ID_CLAS1, FREQUENCY as FREQUENCY\n",
    "FROM `rmf2gcp.RawData.Workflow_aggregado`\n",
    "limit 3105886#310 588 606 ''' # corre en mi local y pesa 56MB %1 del total de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat = get_data_BQ(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_CTE       int64\n",
      "ID_CLAS1     int64\n",
      "FREQUENCY    int64\n",
      "dtype: object\n",
      "(3105886, 3)\n"
     ]
    }
   ],
   "source": [
    "print(final_stat.dtypes)\n",
    "print(final_stat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$readArrowStreamFromFile$2(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|  ID_CTE|ID_CLAS1|FREQUENCY|\n",
      "+--------+--------+---------+\n",
      "| 8913174|  856047|        6|\n",
      "|12064659|  380284|        6|\n",
      "|  879771|  319064|        8|\n",
      "| 7522981|  224025|        6|\n",
      "|14727490|  224057|        7|\n",
      "+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_stat = spark.createDataFrame(final_stat)\n",
    "final_stat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "final_stat.count()\n",
    "print(type(final_stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (final_stat\n",
    "    .select(\n",
    "        'ID_CTE',\n",
    "        'ID_CLAS1',\n",
    "        'FREQUENCY',\n",
    "    )\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data set to test and train for measuring the performance of the ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the recommendation model using ALS on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 2.1386147468410406\n"
     ]
    }
   ],
   "source": [
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=2, regParam=0.01, \n",
    "          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n",
    "          coldStartStrategy=\"drop\",\n",
    "          implicitPrefs=True)\n",
    "\n",
    "model = als.fit(ratings)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+------------+\n",
      "|  ID_CTE|ID_CLAS1|FREQUENCY|  prediction|\n",
      "+--------+--------+---------+------------+\n",
      "| 1397905|  212010|        1|2.4343958E-9|\n",
      "|35269436|  212010|        1|2.4343958E-9|\n",
      "|40116259|  212010|        1|2.4343958E-9|\n",
      "|13853352|  212010|        1|2.4343958E-9|\n",
      "|37916553|  212010|        1|2.4343958E-9|\n",
      "+--------+--------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of ALS Model in PySpark realization are following:\n",
    "\n",
    "##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n",
    "##### rank is the number of latent factors in the model.\n",
    "##### maxIter is the maximum number of iterations to run.\n",
    "##### regParam specifies the regularization parameter in ALS.\n",
    "##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate top 10 Item recommendations for each user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649377\n",
      "+------+--------------------+\n",
      "|ID_CTE|     recommendations|\n",
      "+------+--------------------+\n",
      "| 13832|[[105001, 0.01867...|\n",
      "| 18654|[[224025, 4.90615...|\n",
      "| 22097|[[313096, 0.07093...|\n",
      "| 36525|[[323073, 0.18571...|\n",
      "| 41751|[[314063, 0.01039...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs = model.recommendForAllUsers(10)\n",
    "print(userRecs.count())\n",
    "userRecs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID_CTE=13832, recommendations=[Row(ID_CLAS1=105001, rating=0.018674740567803383), Row(ID_CLAS1=106061, rating=0.017564326524734497), Row(ID_CLAS1=318009, rating=0.01661280170083046), Row(ID_CLAS1=313155, rating=0.016564249992370605), Row(ID_CLAS1=102164, rating=0.015708401799201965), Row(ID_CLAS1=105071, rating=0.013289724476635456), Row(ID_CLAS1=413214, rating=0.012968757189810276), Row(ID_CLAS1=224017, rating=0.01178194023668766), Row(ID_CLAS1=856230, rating=0.010681500658392906), Row(ID_CLAS1=101027, rating=0.010500011965632439)]),\n",
       " Row(ID_CTE=18654, recommendations=[Row(ID_CLAS1=224025, rating=0.0004906158428639174), Row(ID_CLAS1=224065, rating=0.00044970453018322587), Row(ID_CLAS1=105007, rating=0.0003991244302596897), Row(ID_CLAS1=381009, rating=0.0003856293042190373), Row(ID_CLAS1=105074, rating=0.0003786047163885087), Row(ID_CLAS1=701305, rating=0.00036178837763145566), Row(ID_CLAS1=314156, rating=0.0003468487993814051), Row(ID_CLAS1=106055, rating=0.00033687823452055454), Row(ID_CLAS1=866212, rating=0.00027172910631634295), Row(ID_CLAS1=104014, rating=0.0002601128944661468)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     recommendations|\n",
      "+--------------------+\n",
      "|[[105001, 0.01867...|\n",
      "|[[224025, 4.90615...|\n",
      "|[[313096, 0.07093...|\n",
      "|[[323073, 0.18571...|\n",
      "|[[314063, 0.01039...|\n",
      "|[[701305, 0.00194...|\n",
      "|[[314063, 0.62439...|\n",
      "|[[313096, 0.00984...|\n",
      "|[[290059, 0.03170...|\n",
      "|[[314063, 0.32990...|\n",
      "|[[106059, 0.11584...|\n",
      "|[[319064, 0.30891...|\n",
      "|[[313155, 0.00674...|\n",
      "|[[314129, 1.02811...|\n",
      "|[[106001, 0.08439...|\n",
      "|[[313155, 0.00290...|\n",
      "|[[106059, 0.06469...|\n",
      "|[[314129, 0.00993...|\n",
      "|[[314129, 0.08076...|\n",
      "|[[291059, 0.00213...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs[['recommendations']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the recommendations and get them in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|ID_CTE|     recommendations|\n",
      "+------+--------------------+\n",
      "| 13832|[105001, 0.01867474]|\n",
      "| 13832|[106061, 0.017564...|\n",
      "| 13832|[318009, 0.016612...|\n",
      "| 13832|[313155, 0.01656425]|\n",
      "+------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "userRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\n",
    "userRecs1.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Breaking down reach recommendation to separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecs1= userRecs1.select('ID_CTE', 'recommendations.*')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|ID_CTE|ID_CLAS1|     rating|\n",
      "+------+--------+-----------+\n",
      "| 13832|  105001| 0.01867474|\n",
      "| 13832|  106061|0.017564327|\n",
      "+------+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs1.show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26493770"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Output back to the Remote Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_CTE: int, ID_CLAS1: int, rating: float]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stat = userRecs1.toPandas()\n",
    "userRecs1.unpersist(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_gbq\n",
      "  Downloading https://files.pythonhosted.org/packages/53/f3/3100eb9332c62c5e5ac486d5421965da23a0b92012825bfbb372b7f8d508/pandas_gbq-0.13.2-py3-none-any.whl\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (41.4.0)\n",
      "Collecting pydata-google-auth (from pandas_gbq)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/dc/be321b769b761ec2640f1e4561c2953dd6a4a3efe6b10b5781774c71177a/pydata_google_auth-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=0.19.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.0.3)\n",
      "Requirement already satisfied: google-cloud-bigquery>=1.11.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.24.0)\n",
      "Requirement already satisfied: google-auth in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas_gbq) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth-oauthlib->pandas_gbq) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (1.17.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_gbq) (2.8.0)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.17.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (3.12.2)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (0.5.0)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.15.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-cloud-bigquery>=1.11.1->pandas_gbq) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth->pandas_gbq) (4.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (2.23.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (3.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas_gbq) (1.51.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas_gbq) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas_gbq) (1.24.2)\n",
      "Installing collected packages: pydata-google-auth, pandas-gbq\n",
      "Successfully installed pandas-gbq-0.13.2 pydata-google-auth-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = 'Resultados.test_gcp_cluster_10_junio_2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [05:46, 346.94s/it]\n"
     ]
    }
   ],
   "source": [
    "final_stat.to_gbq(table_id, project_id='rmf2gcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat.to_csv('test_gcp_cluster_10_junio_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://test_gcp_cluster_10_junio_2020.csv [Content-Type=text/csv]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][919.4 MiB/919.4 MiB]   63.7 MiB/s                                   \n",
      "Operation completed over 1 objects/919.4 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp test_gcp_cluster_10_junio_2020.csv gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: test_gcp_cluster_10_junio_2020.csv (deflated 70%)\n"
     ]
    }
   ],
   "source": [
    "!zip test_gcp_cluster_10_junio_2020.csv.zip test_gcp_cluster_10_junio_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t\troot\n",
      "boot\t\trun\n",
      "copyright\tsbin\n",
      "dev\t\tsnap\n",
      "etc\t\tsparkmonitor_kernelextension.log\n",
      "hadoop\t\tsrv\n",
      "home\t\tsys\n",
      "initrd.img\ttest\n",
      "initrd.img.old\ttest_gcp_cluster_10_junio_2020.csv\n",
      "lib\t\ttest_gcp_cluster_10_junio_2020.csv.zip\n",
      "lib64\t\ttmp\n",
      "lost+found\tusr\n",
      "media\t\tvar\n",
      "mnt\t\tvmlinuz\n",
      "opt\t\tvmlinuz.old\n",
      "proc\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://test_gcp_cluster_10_junio_2020.csv.zip [Content-Type=application/zip]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][273.1 MiB/273.1 MiB]                                                \n",
      "Operation completed over 1 objects/273.1 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp test_gcp_cluster_10_junio_2020.csv.zip gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r test_modelos\n",
    "!mkdir test_modelos_gcp\n",
    "!chmod 777 test_modelos_gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alsWML = pipeline.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alsWML.save('/test_modelos_gcp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1221244\n",
      "drwxr-xr-x  29 root root        4096 Jun 10 12:03 .\n",
      "drwxr-xr-x  29 root root        4096 Jun 10 12:03 ..\n",
      "drwx------   3 root root        4096 Jun 10 09:48 .config\n",
      "drwxr-xr-x   2 root root        4096 May 28 05:01 bin\n",
      "drwxr-xr-x   4 root root        4096 May 28 04:59 boot\n",
      "-rw-r--r--   1 root root         646 Sep 10  2019 copyright\n",
      "drwxr-xr-x  16 root root        3880 Jun 10 09:22 dev\n",
      "drwxr-xr-x 120 root root       12288 Jun 10 09:23 etc\n",
      "drwxr-xr-x   2 root root        4096 Jun 10 11:57 foo\n",
      "drwxrwxr-x   7 root hadoop      4096 Jun 10 09:22 hadoop\n",
      "drwxr-xr-x   3 root root        4096 Jun 10 09:22 home\n",
      "lrwxrwxrwx   1 root root          30 May 21 17:47 initrd.img -> boot/initrd.img-5.3.0-1020-gcp\n",
      "lrwxrwxrwx   1 root root          30 May 21 17:47 initrd.img.old -> boot/initrd.img-5.3.0-1020-gcp\n",
      "drwxr-xr-x  22 root root        4096 May 28 05:07 lib\n",
      "drwxr-xr-x   2 root root        4096 May 21 17:39 lib64\n",
      "drwx------   2 root root       16384 May 21 17:45 lost+found\n",
      "drwxr-xr-x   2 root root        4096 May 21 17:39 media\n",
      "drwxr-xr-x   2 root root        4096 May 21 17:39 mnt\n",
      "drwxr-xr-x   9 root root        4096 Jun 10 09:22 opt\n",
      "dr-xr-xr-x 204 root root           0 Jun 10 09:21 proc\n",
      "drwx------  11 root root        4096 Jun 10 09:40 root\n",
      "drwxr-xr-x  37 root root        1200 Jun 10 11:22 run\n",
      "drwxr-xr-x   2 root root       12288 Jun 10 09:22 sbin\n",
      "drwxr-xr-x   6 root root        4096 May 28 04:58 snap\n",
      "-rw-r--r--   1 root root         772 Jun 10 11:43 sparkmonitor_kernelextension.log\n",
      "drwxr-xr-x   2 root root        4096 May 21 17:39 srv\n",
      "dr-xr-xr-x  13 root root           0 Jun 10 09:21 sys\n",
      "drwxr-xr-x   2 root root        4096 Jun 10 11:37 test\n",
      "-rw-r--r--   1 root root   964004734 Jun 10 11:27 test_gcp_cluster_10_junio_2020.csv\n",
      "-rw-r--r--   1 root root   286400350 Jun 10 11:41 test_gcp_cluster_10_junio_2020.csv.zip\n",
      "drwxrwxrwx   2 root root        4096 Jun 10 12:03 test_modelos\n",
      "drwxrwxrwx   2 root root        4096 Jun 10 12:03 test_modelos_gcp\n",
      "drwxrwxrwt  40 root root        4096 Jun 10 12:04 tmp\n",
      "drwxr-xr-x  10 root root        4096 Jun 10 09:22 usr\n",
      "drwxr-xr-x  13 root root        4096 May 21 17:43 var\n",
      "lrwxrwxrwx   1 root root          27 May 21 17:47 vmlinuz -> boot/vmlinuz-5.3.0-1020-gcp\n",
      "lrwxrwxrwx   1 root root          27 May 21 17:47 vmlinuz.old -> boot/vmlinuz-5.3.0-1020-gcp\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp test_gcp_cluster_10_junio_2020.csv.zip gs://resultadosrmf2/prueba_gcp_01porciento/test_local_10_junio_2020.csv.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
