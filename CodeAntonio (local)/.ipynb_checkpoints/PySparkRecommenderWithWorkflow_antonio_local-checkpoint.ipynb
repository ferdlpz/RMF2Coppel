{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow demonstration with a recommender engine on a sampled dataset from Transactions.csv using ALS Model\n",
    "### This is the notebook for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement dsx_ml (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for dsx_ml\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# LIBRARIES\n",
    "#!pip install pyspark\n",
    "#!pip install --upgrade google-cloud-bigquery[pandas]\n",
    "#!pip install pyspark[sql] #PARSEO RAPIDO DE PANDAS A SPARK RDDSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries and starting the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "#!export ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "!echo $ARROW_PRE_0_15_IPC_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antonio/Desktop/github/RMF2Coppel/BQkeys/antoniobq_key.json\n",
      "/usr/lib/jvm/java-8-openjdk-amd64\n",
      "/opt/spark\n",
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1ubuntu1-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!echo $GOOGLE_APPLICATION_CREDENTIALS\n",
    "!echo $JAVA_HOME\n",
    "!echo $SPARK_HOME\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add asset from remote connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CTE</th>\n",
       "      <th>ID_CLAS1</th>\n",
       "      <th>FREQUENCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31936884</td>\n",
       "      <td>541060</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5916508</td>\n",
       "      <td>314063</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13173248</td>\n",
       "      <td>224065</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4532176</td>\n",
       "      <td>314063</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18164173</td>\n",
       "      <td>314063</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25655734</td>\n",
       "      <td>594244</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32812328</td>\n",
       "      <td>104014</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3000656</td>\n",
       "      <td>106061</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36507674</td>\n",
       "      <td>420083</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27660076</td>\n",
       "      <td>477092</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_CTE  ID_CLAS1  FREQUENCY\n",
       "0  31936884    541060         10\n",
       "1   5916508    314063          6\n",
       "2  13173248    224065          9\n",
       "3   4532176    314063          7\n",
       "4  18164173    314063         13\n",
       "5  25655734    594244          6\n",
       "6  32812328    104014          6\n",
       "7   3000656    106061          7\n",
       "8  36507674    420083          6\n",
       "9  27660076    477092         10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  requests, os, io #dsx_core_utils\n",
    "#final_stat = None\n",
    "final_stat = pd.read_csv('Workflow_aggregado.csv')\n",
    "final_stat.columns = ['ID_CTE', 'ID_CLAS1', 'FREQUENCY']\n",
    "final_stat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from GCP- BQ\n",
    "#from google.cloud import bigquery\n",
    "#def get_data_BQ(sql):\n",
    "#    client = bigquery.Client()\n",
    "#    df = client.query(sql).to_dataframe()\n",
    "#    return(df)\n",
    "#sql =  '''SELECT ID_CTE as ID_CTE, ID_FAM as ID_CLAS1, FREQUENCY as FREQUENCY\n",
    "#FROM `rmf2gcp.RawData.Workflow_aggregado`\n",
    "#limit 3105886#310 588 606 ''' # corre en mi local y pesa 56MB %1 del total de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = get_data_BQ(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_CTE       int64\n",
      "ID_CLAS1     int64\n",
      "FREQUENCY    int64\n",
      "dtype: object\n",
      "(3105886, 3)\n"
     ]
    }
   ],
   "source": [
    "print(final_stat.dtypes)\n",
    "print(final_stat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|  ID_CTE|ID_CLAS1|FREQUENCY|\n",
      "+--------+--------+---------+\n",
      "|31936884|  541060|       10|\n",
      "| 5916508|  314063|        6|\n",
      "|13173248|  224065|        9|\n",
      "| 4532176|  314063|        7|\n",
      "|18164173|  314063|       13|\n",
      "+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_stat = spark.createDataFrame(final_stat)\n",
    "final_stat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105886"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.sql.types import IntegerType\n",
    "#for i in final_stat.columns:\n",
    " #   final_stat = final_stat.withColumn(i, final_stat[i].cast(IntegerType()))\n",
    "final_stat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (final_stat\n",
    "    .select(\n",
    "        'ID_CTE',\n",
    "        'ID_CLAS1',\n",
    "        'FREQUENCY',\n",
    "    )\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings.limit(310588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105886"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data set to test and train for measuring the performance of the ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the recommendation model using ALS on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=2, regParam=0.01, \n",
    "          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n",
    "          coldStartStrategy=\"drop\",\n",
    "          implicitPrefs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 2.011249846550589\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(ratings)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------+\n",
      "|  ID_CTE|ID_CLAS1|FREQUENCY|    prediction|\n",
      "+--------+--------+---------+--------------+\n",
      "|26611533|  212010|        1|-1.33456455E-8|\n",
      "|24858262|  212010|        1|   4.194458E-9|\n",
      "|39868047|  212010|        1|   4.194458E-9|\n",
      "|32473582|  212010|        1|   4.194458E-9|\n",
      "|14107606|  212010|        1|   3.229041E-6|\n",
      "+--------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of ALS Model in PySpark realization are following:\n",
    "\n",
    "##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n",
    "##### rank is the number of latent factors in the model.\n",
    "##### maxIter is the maximum number of iterations to run.\n",
    "##### regParam specifies the regularization parameter in ALS.\n",
    "##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate top 10 Item recommendations for each user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649183\n",
      "+------+--------------------+\n",
      "|ID_CTE|     recommendations|\n",
      "+------+--------------------+\n",
      "| 19553|[[106059, 0.01266...|\n",
      "| 19984|[[104014, 0.00532...|\n",
      "| 29719|[[314129, 0.01303...|\n",
      "| 37307|[[860048, 0.27645...|\n",
      "| 38723|[[102011, 4.50272...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs = model.recommendForAllUsers(10)\n",
    "print(userRecs.count())\n",
    "userRecs.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method recommendForAllUsers in module pyspark.ml.recommendation:\n",
      "\n",
      "recommendForAllUsers(numItems) method of pyspark.ml.recommendation.ALSModel instance\n",
      "    Returns top `numItems` items recommended for each user, for all users.\n",
      "    \n",
      "    :param numItems: max number of recommendations for each user\n",
      "    :return: a DataFrame of (userCol, recommendations), where recommendations are\n",
      "             stored as an array of (itemCol, rating) Rows.\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.recommendForAllUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     recommendations|\n",
      "+--------------------+\n",
      "|[[106059, 0.01266...|\n",
      "|[[104014, 0.00532...|\n",
      "|[[314129, 0.01303...|\n",
      "|[[860048, 0.27645...|\n",
      "|[[102011, 4.50272...|\n",
      "|[[102089, 0.05551...|\n",
      "|[[314156, 0.56780...|\n",
      "|[[102089, 0.03627...|\n",
      "|[[314063, 0.04459...|\n",
      "|[[319064, 0.03723...|\n",
      "|[[380284, 0.01422...|\n",
      "|[[701305, 7.63655...|\n",
      "|[[106003, 0.60024...|\n",
      "|[[224041, 0.01021...|\n",
      "|[[106055, 0.80115...|\n",
      "|[[104014, 0.20452...|\n",
      "|[[319064, 0.06844...|\n",
      "|[[224065, 0.32506...|\n",
      "|[[106061, 0.01819...|\n",
      "|[[121001, 0.16221...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs[['recommendations']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method take in module pyspark.sql.dataframe:\n",
      "\n",
      "take(num) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "    \n",
      "    >>> df.take(2)\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID_CTE=19553, recommendations=[Row(ID_CLAS1=106059, rating=0.012666523456573486), Row(ID_CLAS1=104015, rating=0.012505062855780125), Row(ID_CLAS1=224017, rating=0.012294436804950237), Row(ID_CLAS1=319064, rating=0.01215100847184658), Row(ID_CLAS1=106061, rating=0.011899995617568493), Row(ID_CLAS1=224025, rating=0.011808079667389393), Row(ID_CLAS1=318009, rating=0.010645071975886822), Row(ID_CLAS1=423132, rating=0.008973229676485062), Row(ID_CLAS1=105074, rating=0.008193274959921837), Row(ID_CLAS1=101004, rating=0.007982431910932064)]),\n",
       " Row(ID_CTE=19984, recommendations=[Row(ID_CLAS1=104014, rating=0.005321032367646694), Row(ID_CLAS1=104015, rating=0.004974351730197668), Row(ID_CLAS1=224025, rating=0.004570256918668747), Row(ID_CLAS1=291059, rating=0.004364300053566694), Row(ID_CLAS1=224017, rating=0.004133342765271664), Row(ID_CLAS1=869212, rating=0.0032216128893196583), Row(ID_CLAS1=224065, rating=0.003081432543694973), Row(ID_CLAS1=314156, rating=0.0025232555344700813), Row(ID_CLAS1=106001, rating=0.0023891639430075884), Row(ID_CLAS1=787254, rating=0.0022626048885285854)]),\n",
       " Row(ID_CTE=29719, recommendations=[Row(ID_CLAS1=314129, rating=0.01303502731025219), Row(ID_CLAS1=104015, rating=0.01077840942889452), Row(ID_CLAS1=313155, rating=0.009336080402135849), Row(ID_CLAS1=224025, rating=0.007873447611927986), Row(ID_CLAS1=862009, rating=0.007644485216587782), Row(ID_CLAS1=104014, rating=0.007543453015387058), Row(ID_CLAS1=319064, rating=0.007024914026260376), Row(ID_CLAS1=224017, rating=0.006534466985613108), Row(ID_CLAS1=869212, rating=0.006194644141942263), Row(ID_CLAS1=864212, rating=0.0059182848781347275)]),\n",
       " Row(ID_CTE=37307, recommendations=[Row(ID_CLAS1=860048, rating=0.27645498514175415), Row(ID_CLAS1=224065, rating=0.24172967672348022), Row(ID_CLAS1=106010, rating=0.19125314056873322), Row(ID_CLAS1=413238, rating=0.12767764925956726), Row(ID_CLAS1=314063, rating=0.11099892854690552), Row(ID_CLAS1=106055, rating=0.10762359946966171), Row(ID_CLAS1=380284, rating=0.1050233244895935), Row(ID_CLAS1=862003, rating=0.10422034561634064), Row(ID_CLAS1=313096, rating=0.10327751934528351), Row(ID_CLAS1=291059, rating=0.09821733087301254)]),\n",
       " Row(ID_CTE=38723, recommendations=[Row(ID_CLAS1=102011, rating=0.0004502724332269281), Row(ID_CLAS1=106061, rating=0.00038859000778757036), Row(ID_CLAS1=290059, rating=0.0003748355957213789), Row(ID_CLAS1=101027, rating=0.0003094978164881468), Row(ID_CLAS1=106054, rating=0.00029705403721891344), Row(ID_CLAS1=306189, rating=0.00026608811458572745), Row(ID_CLAS1=105074, rating=0.0002557287225499749), Row(ID_CLAS1=102089, rating=0.0002518606779631227), Row(ID_CLAS1=314063, rating=0.00024912896333262324), Row(ID_CLAS1=318009, rating=0.00021131770336069167)])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(help(userRecs.take))\n",
    "userRecs.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the recommendations and get them in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|ID_CTE|     recommendations|\n",
      "+------+--------------------+\n",
      "| 19553|[106059, 0.012666...|\n",
      "| 19553|[104015, 0.012505...|\n",
      "| 19553|[224017, 0.012294...|\n",
      "| 19553|[319064, 0.012151...|\n",
      "| 19553|[106061, 0.011899...|\n",
      "| 19553|[224025, 0.01180808]|\n",
      "| 19553|[318009, 0.010645...|\n",
      "| 19553|[423132, 0.00897323]|\n",
      "| 19553|[105074, 0.008193...|\n",
      "| 19553|[101004, 0.007982...|\n",
      "| 19984|[104014, 0.005321...|\n",
      "| 19984|[104015, 0.004974...|\n",
      "| 19984|[224025, 0.004570...|\n",
      "| 19984| [291059, 0.0043643]|\n",
      "| 19984|[224017, 0.004133...|\n",
      "| 19984|[869212, 0.003221...|\n",
      "| 19984|[224065, 0.003081...|\n",
      "| 19984|[314156, 0.002523...|\n",
      "| 19984|[106001, 0.002389...|\n",
      "| 19984|[787254, 0.002262...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "userRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\n",
    "userRecs1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_CTE: bigint, ID_CLAS1: bigint, FREQUENCY: bigint]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ratings.unpersist(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Breaking down reach recommendation to separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecs1= userRecs1.select('ID_CTE', 'recommendations.*')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------+\n",
      "|ID_CTE|ID_CLAS1|      rating|\n",
      "+------+--------+------------+\n",
      "| 19553|  106059| 0.012666523|\n",
      "| 19553|  104015| 0.012505063|\n",
      "| 19553|  224017| 0.012294437|\n",
      "| 19553|  319064|0.0121510085|\n",
      "| 19553|  106061| 0.011899996|\n",
      "| 19553|  224025|  0.01180808|\n",
      "| 19553|  318009| 0.010645072|\n",
      "| 19553|  423132|  0.00897323|\n",
      "| 19553|  105074| 0.008193275|\n",
      "| 19553|  101004| 0.007982432|\n",
      "| 19984|  104014|0.0053210324|\n",
      "| 19984|  104015|0.0049743517|\n",
      "| 19984|  224025| 0.004570257|\n",
      "| 19984|  291059|   0.0043643|\n",
      "| 19984|  224017|0.0041333428|\n",
      "| 19984|  869212| 0.003221613|\n",
      "| 19984|  224065|0.0030814325|\n",
      "| 19984|  314156|0.0025232555|\n",
      "| 19984|  106001| 0.002389164|\n",
      "| 19984|  787254| 0.002262605|\n",
      "+------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs1.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26491830"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Output back to the Remote Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat = userRecs1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_CTE: int, ID_CLAS1: int, rating: float]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs1.unpersist(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat.to_csv('test_local_10_junio_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: test_local_10_junio_2020.csv (deflated 70%)\n"
     ]
    }
   ],
   "source": [
    "!zip test_local_10_junio_2020.csv.zip test_local_10_junio_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos los resultados de momento porque son bastente grandes aun compresos como para guardarlos en el repo \n",
    "!rm test_local_10_junio_2020.csv\n",
    "!rm test_local_10_junio_2020.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alsWML = pipeline.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alsWML.save('/home/antonio/Desktop/github/RMF2Coppel/CodeAntonio (local)/Experiment_local/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :D END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
